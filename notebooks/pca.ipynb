{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262c869b",
   "metadata": {},
   "source": [
    "# PCA for NSL-KDD\n",
    "\n",
    "Dimensionality reduction built on top of the NSL-KDD preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and plotting defaults\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9631d4",
   "metadata": {},
   "source": [
    "## Load preprocessed datasets\n",
    "Use the existing normalized tables as the baseline for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/test splits and inspect class balance\n",
    "train_df = pd.read_csv(DATA_DIR / 'preproc_kdd_train.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'preproc_kdd_test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape} | Test shape: {test_df.shape}\")\n",
    "print(\"Train attack distribution (top 5):\")\n",
    "print(train_df['attack_type'].value_counts().head())\n",
    "print(\"Test attack distribution (top 5):\")\n",
    "print(test_df['attack_type'].value_counts().head())\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090310d",
   "metadata": {},
   "source": [
    "## Feature scaling & label prep\n",
    "Re-apply `StandardScaler` so PCA sees zero-mean/unit-variance features, and create binary attack labels for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bcf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in train_df.columns if col != 'attack_type']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "X_test = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "y_train = (train_df['attack_type'] != 'normal').astype(int)\n",
    "y_test = (test_df['attack_type'] != 'normal').astype(int)\n",
    "\n",
    "print(f\"Scaled feature matrix shape (train/test): {X_train.shape} / {X_test.shape}\")\n",
    "print(f\"Attack rate train/test: {y_train.mean():.2%} / {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d68be37",
   "metadata": {},
   "source": [
    "## PCA variance analysis\n",
    "Fit a full PCA to understand the variance curve and pick the number of components hitting ~95% cumulative variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db758b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on the standardized training set to inspect explained variance\n",
    "pca_full = PCA().fit(X_train)\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained)\n",
    "n_components_95 = int(np.argmax(cum_explained >= 0.95) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(range(1, len(cum_explained) + 1), cum_explained, marker='o')\n",
    "ax.axhline(0.95, color='r', linestyle='--', label='95% target')\n",
    "ax.axvline(n_components_95, color='g', linestyle='--', label=f'{n_components_95} comps')\n",
    "ax.set_xlabel('Number of principal components')\n",
    "ax.set_ylabel('Cumulative explained variance')\n",
    "ax.set_title('PCA cumulative variance (train set)')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Components needed for >=95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423539d3",
   "metadata": {},
   "source": [
    "## Transform & save PCA datasets\n",
    "Project both splits into the PCA space and persist the tables following the `PCA-[Name]` convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PCA transformer using the selected number of components\n",
    "pca = PCA(n_components=n_components_95, random_state=RANDOM_STATE)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "pca_columns = [f'pc{i:02d}' for i in range(1, X_train_pca.shape[1] + 1)]\n",
    "train_pca_df = pd.DataFrame(X_train_pca, columns=pca_columns)\n",
    "train_pca_df['attack_type'] = train_df['attack_type'].values\n",
    "\n",
    "test_pca_df = pd.DataFrame(X_test_pca, columns=pca_columns)\n",
    "test_pca_df['attack_type'] = test_df['attack_type'].values\n",
    "\n",
    "train_path = DATA_DIR / 'PCA-nsl_kdd_train.csv'\n",
    "test_path = DATA_DIR / 'PCA-nsl_kdd_test.csv'\n",
    "train_pca_df.to_csv(train_path, index=False)\n",
    "test_pca_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Saved PCA train set -> {train_path} ({train_pca_df.shape})\")\n",
    "print(f\"Saved PCA test set  -> {test_path} ({test_pca_df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2aaa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pca.components_\n",
    "feature_names = train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b927328",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pc_loadings in enumerate(loadings):\n",
    "    # Get the absolute values of loadings for the current PC\n",
    "    abs_loadings = np.abs(pc_loadings)\n",
    "    \n",
    "    # Get the indices of features sorted by their absolute loadings (descending)\n",
    "    sorted_indices = np.argsort(abs_loadings)[::-1]\n",
    "    \n",
    "    print(f\"\\nMost influential features for PC{i+1}:\")\n",
    "    for j in range(min(5, len(sorted_indices))): # Print top 5 or fewer\n",
    "        feature_index = sorted_indices[j]\n",
    "        feature_name = feature_names[feature_index]\n",
    "        loading_value = pc_loadings[feature_index]\n",
    "        print(f\"- {feature_name}: {loading_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
